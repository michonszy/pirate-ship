# namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: vulnerable-app

---
# configmap.yaml - Using ConfigMap for sensitive data instead of Secrets (vulnerability)
apiVersion: v1
kind: ConfigMap
metadata:
  name: db-credentials
  namespace: vulnerable-app
data:
  DB_USER: "admin"
  DB_PASSWORD: "password123"  # Should be in a Secret, not a ConfigMap
  MONGODB_CONNECTION: "mongodb://admin:password123@mongodb-service:27017/vulnapp"
  API_SECRET: "api-key-9875-insecure-k8s-ctf-1234"
  FLAG: "FLAG{sensitive_data_in_configmap}"

---
# secret.yaml - Base64 encoded but not properly safeguarded
apiVersion: v1
kind: Secret
metadata:
  name: application-secrets
  namespace: vulnerable-app
type: Opaque
data:
  admin-password: YWRtaW5fcGFzc3dvcmRfMTIzCg==
  api-key: c2VjcmV0LWFwaS1rZXktZm9yLXZ1bG5lcmFibGUtYXBwCg==
  flag: RkxBR3tzZWNyZXRzX2JhZGx5X21hbmFnZWR9Cg==

---
# vulnerable-serviceaccount.yaml - Overly permissive RBAC
apiVersion: v1
kind: ServiceAccount
metadata:
  name: vulnerable-sa
  namespace: vulnerable-app
  annotations:
    ctf.flag/overly_permissive_rbac: "FLAG{overly_permissive_rbac}"


---
# cluster-role.yaml - Overly permissive RBAC
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: vulnerable-cluster-role
rules:
- apiGroups: [""]
  resources: ["pods", "pods/exec", "pods/log", "secrets", "configmaps", "namespaces"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["apps"]
  resources: ["deployments", "daemonsets", "statefulsets"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]
- apiGroups: ["batch"]
  resources: ["jobs", "cronjobs"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

---
# rolebinding.yaml - Binding the overly permissive role to the SA
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: vulnerable-cluster-rolebinding
subjects:
- kind: ServiceAccount
  name: vulnerable-sa
  namespace: vulnerable-app
roleRef:
  kind: ClusterRole
  name: vulnerable-cluster-role
  apiGroup: rbac.authorization.k8s.io

---
# frontend-deployment.yaml - Insecure workload configuration
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
  namespace: vulnerable-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      serviceAccountName: vulnerable-sa  # Using overly permissive SA
      containers:
      - name: frontend
        image: michonszy/k8s-ctf-frontend:v1  # Using vulnerable image
        ports:
        - containerPort: 3000
        securityContext:
          privileged: true  # Highly insecure
          capabilities:
            add: ["ALL"]  # Excessive capabilities
          runAsUser: 0  # Running as root
          readOnlyRootFilesystem: false  # Writable filesystem
        env:
        - name: BACKEND_URL
          value: "http://backend-service:5000"
        - name: API_KEY
          valueFrom:
            configMapKeyRef:  # Should be secretKeyRef
              name: db-credentials
              key: API_SECRET
        volumeMounts:
        - name: host-fs
          mountPath: /host  # Dangerous host filesystem mount
        resources:
          limits:
            cpu: "200m"
            memory: "256Mi"
          requests:
            cpu: "100m"
            memory: "128Mi"
      volumes:
      - name: host-fs
        hostPath:
          path: /  # Mounting entire host filesystem (extremely dangerous)
          type: Directory

---
# backend-deployment.yaml - Multiple security issues
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  namespace: vulnerable-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
    spec:
      serviceAccountName: vulnerable-sa  # Overly permissive SA
      containers:
      - name: backend
        image: michonszy/k8s-ctf-backend:v1  # Vulnerable image from earlier
        ports:
        - containerPort: 5000
        securityContext:
          allowPrivilegeEscalation: true  # Allows privilege escalation
          runAsUser: 0  # Running as root
          readOnlyRootFilesystem: false  # Writable filesystem
        env:
        - name: DB_USER
          valueFrom:
            configMapKeyRef:
              name: db-credentials
              key: DB_USER
        - name: DB_PASSWORD
          valueFrom:
            configMapKeyRef:  # Should use secretKeyRef
              name: db-credentials
              key: DB_PASSWORD
        - name: DEBUG
          value: "true"  # Exposing debug information
        - name: FLAG
          value: "FLAG{insecure_k8s_deployment}"
        volumeMounts:
        - name: tmp-volume
          mountPath: /tmp
        resources:
          limits:
            cpu: "200m"
            memory: "256Mi"
          requests:
            cpu: "100m"
            memory: "128Mi"
      volumes:
      - name: tmp-volume
        emptyDir: {}

---
# mongodb-statefulset.yaml - Persistent storage with insecure defaults
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongodb
  namespace: vulnerable-app
spec:
  serviceName: "mongodb"
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
      - name: mongodb
        image: mongo:4.4
        ports:
        - containerPort: 27017
        env:
        - name: MONGO_INITDB_ROOT_USERNAME
          valueFrom:
            configMapKeyRef:  # Should use secretKeyRef
              name: db-credentials
              key: DB_USER
        - name: MONGO_INITDB_ROOT_PASSWORD
          valueFrom:
            configMapKeyRef:  # Should use secretKeyRef
              name: db-credentials
              key: DB_PASSWORD
        volumeMounts:
        - name: mongodb-data
          mountPath: /data/db
        resources:
          limits:
            cpu: "500m"
            memory: "512Mi"
          requests:
            cpu: "300m"
            memory: "256Mi"
  volumeClaimTemplates:
  - metadata:
      name: mongodb-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 1Gi

---
# frontend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
  namespace: vulnerable-app
spec:
  selector:
    app: frontend
  ports:
  - port: 80
    targetPort: 3000
  type: LoadBalancer  # Exposing directly rather than through proper ingress

---
# backend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: backend-service
  namespace: vulnerable-app
  annotations:
    ctf.flag: "FLAG{missing_network_policy}"
spec:
  selector:
    app: backend
  ports:
  - port: 5000
    targetPort: 5000
  type: ClusterIP


---
# mongodb-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: mongodb-service
  namespace: vulnerable-app
spec:
  selector:
    app: mongodb
  ports:
  - port: 27017
    targetPort: 27017
  type: ClusterIP

---
# admin-pod.yaml - Pod with excessive privileges that can be used as part of attack
apiVersion: v1
kind: Pod
metadata:
  name: admin-tools
  namespace: vulnerable-app
  labels:
    app: admin-tools
spec:
  serviceAccountName: vulnerable-sa
  containers:
  - name: admin-tools
    image: alpine:latest
    command: ["sh", "-c", "echo 'FLAG{privileged_admin_pod}' > /root/flag.txt && sleep infinity"]
    securityContext:
      privileged: true
      capabilities:
        add: ["NET_ADMIN", "SYS_ADMIN"]
    resources:
      limits:
        cpu: "100m"
        memory: "128Mi"
      requests:
        cpu: "50m"
        memory: "64Mi"

---
# vulnerable-network-policy.yaml (intentionally missing proper restrictions)
# A properly configured network policy would be crucial but is missing here
# No NetworkPolicy is defined, allowing all pods to communicate with each other
# There should be a policy restricting access to backend from only the frontend

---
# dashboard-deployment.yaml - The actual dashboard pod
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubernetes-dashboard
  namespace: vulnerable-app
  labels:
    k8s-app: kubernetes-dashboard
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: kubernetes-dashboard
  template:
    metadata:
      labels:
        k8s-app: kubernetes-dashboard
    spec:
      serviceAccountName: kubernetes-dashboard
      containers:
      - name: kubernetes-dashboard
        image: kubernetesui/dashboard:v2.7.0
        ports:
        - containerPort: 8443
        volumeMounts:
        - mountPath: /certs
          name: kubernetes-dashboard-certs
        - mountPath: /tmp
          name: tmp-volume
      volumes:
      - name: kubernetes-dashboard-certs
        emptyDir: {}
      - name: tmp-volume
        emptyDir: {}
---
# dashboard-rbac.yaml - Exposing dashboard with admin privileges
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubernetes-dashboard
  namespace: vulnerable-app

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard
  namespace: vulnerable-app
roleRef:
  kind: ClusterRole
  name: cluster-admin  # Giving dashboard admin privileges (very insecure)
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: Secret
metadata:
  name: dashboard-flag
  namespace: vulnerable-app
type: Opaque
data:
  flag: RkxBR3ttaXNjb25maWd1cmVkX2Rhc2hib2FyZH0=

---
# dashboard-service.yaml - Exposing dashboard externally (bad practice)
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-dashboard
  namespace: vulnerable-app
  labels:
    k8s-app: kubernetes-dashboard
spec:
  type: NodePort
  selector:
    k8s-app: kubernetes-dashboard
  ports:
    - name: http
      port: 80
      targetPort: 9090
      nodePort: 30880

---
# Challenge Job that places a flag on nodes
apiVersion: batch/v1
kind: Job
metadata:
  name: node-flag-placement
  namespace: vulnerable-app
spec:
  template:
    spec:
      serviceAccountName: vulnerable-sa
      containers:
      - name: flagplacer
        image: alpine:latest
        command:
        - sh
        - -c
        - "echo 'FLAG{node_compromise_via_hostpath}' > /host/tmp/k8s-node-flag.txt && echo 'Job complete'"
        securityContext:
          privileged: true
        volumeMounts:
        - name: hostpath
          mountPath: /host
      volumes:
      - name: hostpath
        hostPath:
          path: /
      restartPolicy: Never
  backoffLimit: 1

---
# Outdated admission controller configuration (represented as a ConfigMap)
apiVersion: v1
kind: ConfigMap
metadata:
  name: admission-control-config
  namespace: vulnerable-app
data:
  admission-config.yaml: |
    # Intentionally missing crucial admission controllers like:
    # - PodSecurityPolicy
    # - LimitRanger
    # - SecurityContextDeny
    # Flag: FLAG{missing_admission_controls}
---
# stealthy-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: stealthy-job
  namespace: vulnerable-app
spec:
  template:
    spec:
      restartPolicy: Never
      hostPID: true
      serviceAccountName: vulnerable-sa
      containers:
      - name: stealthy
        image: alpine:latest
        securityContext:
          privileged: true
        command: ["/bin/sh", "-c"]
        args:
          - |
            apk add --no-cache util-linux;
            echo "Mar 31 23:59:59 containerd[1234]: FLAG{inadequate_logging}" >> /host/var/log/syslog;
            echo "Stealth job complete"
        volumeMounts:
        - name: host-logs
          mountPath: /host
      volumes:
      - name: host-logs
        hostPath:
          path: /

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: player
  namespace: vulnerable-app
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: player-role
  namespace: vulnerable-app
rules:
- apiGroups: [""]
  resources:
    - pods
  verbs:
    - list
- apiGroups: [""]
  resources:
    - pods/exec
    - pods/log
  verbs:
    - create
- apiGroups: [""]
  resources:
    - configmaps
  verbs:
    - get
    - list
    - watch
- apiGroups: [""]
  resources:
    - secrets
  verbs:
    - get
    - list
    - watch
- apiGroups: [""]
  resources:
    - services
  verbs:
    - get
    - list
    - watch
- apiGroups: [""]
  resources:
    - secrets
  verbs: [""]


---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: player-rolebinding
  namespace: vulnerable-app
subjects:
- kind: ServiceAccount
  name: player
  namespace: vulnerable-app
roleRef:
  kind: Role
  name: player-role
  apiGroup: rbac.authorization.k8s.io

---
# player-sa-token.yaml
apiVersion: v1
kind: Secret
metadata:
  name: player-sa-token
  namespace: vulnerable-app
  annotations:
    kubernetes.io/service-account.name: player
type: kubernetes.io/service-account-token
---
apiVersion: v1
kind: Pod
metadata:
  name: host-inspector
  namespace: vulnerable-app
spec:
  hostPID: true
  hostNetwork: true
  serviceAccountName: vulnerable-sa
  containers:
  - name: alpine
    image: alpine:latest
    command: ["sh", "-c", "sleep infinity"]
    securityContext:
      privileged: true
    volumeMounts:
    - name: host-logs
      mountPath: /host
  volumes:
  - name: host-logs
    hostPath:
      path: /
      type: Directory
